{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have you ever tried to perform a computationaly expensive operation on your computer? Have you been forced to wait a few hours for a result?\n",
      "\n",
      "If so, I'll provide an easy way to scale some of problems, that you can quickly run them on multiple (100?) computers or just a single multicore one, and quickly get the results.\n",
      "\n",
      "Let's start with a simple Spark usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sc.textFile(\"/usr/local/spark-1.6.1/README.md\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.filter(lambda line: \"Spark\" in line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lineLengths = df.map(lambda s: len(s))\n",
      "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
      "totalLength"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's look a simple example that's working on XML file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='book').load('/home/ubuntu/books.xml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2 = df.select(\"author\", \"@id\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df2.filter(df2[\"author\"]> 'D').show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.filter(df['author'] > 21).show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's time to do some serious Machine Learning "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from numpy import array\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(' ')]\n",
      "    return LabeledPoint(values[0], values[1:])\n",
      "\n",
      "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
      "parsedData = data.map(parsePoint)\n",
      "\n",
      "# Build the model\n",
      "model = LogisticRegressionWithSGD.train(parsedData)\n",
      "\n",
      "# Evaluating the model on training data\n",
      "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
      "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
      "print(\"Training Error = \" + str(trainErr))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}